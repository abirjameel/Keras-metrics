{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jameel\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F_measure Scoring function \n",
    "### F_measure scoring function is the harmonic mean of specificity and sensitivity\n",
    "F measure is a function of sensitivity and specificity, sometimes it is quite difficult to find a right model especially when you are grid searching hyperparameters, since the model evaluation keras has to offer is only `accuracy` in classification which does not gives you the best model in terms of specificity and sensitivity, i have come up with a new measure that can be used as model evaluation metric while during the training to asses a model, and when grid search is used it will give you the model which has best f_measure score. \n",
    "f_measure is actually Harmonic mean of sensitivity and specificity, while `f1_score` from sklearn is a Harmonic mean of precision and recall. \n",
    "Initially the function works with only binary classes, in future will modify it for multiclass classification as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def f_measure(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    f_measure: Harmonic mean of specificity and sensitivity, shall be used to calculate score batch wise\n",
    "    during training\n",
    "    **for binary classification only**\n",
    "    @param\n",
    "    y_true: Tensor of actual labels \n",
    "    y_pred: Tensor of predicted labels \n",
    "    @returns \n",
    "    f_measure score for a batch \n",
    "    \"\"\"\n",
    "    def specificity(y_true, y_pred):\n",
    "        \"\"\"Compute the confusion matrix for a set of predictions.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred   : predicted values for a batch if samples (must be binary: 0 or 1)\n",
    "        y_true   : correct values for the set of samples used (must be binary: 0 or 1)\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        out : the specificity\n",
    "        \"\"\"\n",
    "        neg_y_true = 1 - y_true\n",
    "        neg_y_pred = 1 - y_pred\n",
    "        fp = K.sum(neg_y_true * y_pred)\n",
    "        tn = K.sum(neg_y_true * neg_y_pred)\n",
    "        \n",
    "        specificity = tn / (tn + fp + K.epsilon())\n",
    "        return specificity\n",
    "    \n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        \n",
    "        Only computes a batch-wise average of recall.\n",
    "        \n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    \n",
    "    specificity = specificity(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((specificity * recall)/(specificity + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Computes the F score.\n",
    "     The F1 score is harmonic mean of precision and recall.\n",
    "     it is computed as a batch-wise average.\n",
    "     This is can be used for multi-label classification. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "         Only computes a batch-wise average of precision.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    f1_score = 2 * (p * r) / (p + r + K.epsilon())\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =      [[0., 1., 0.],\n",
    "       [1., 0., 0.],\n",
    "       [1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 0., 1.],\n",
    "       [1., 0., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 1., 0.],\n",
    "       [1., 0., 0.]]\n",
    "y_predicted = [[0., 1., 0.],\n",
    "       [1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 0., 1.],\n",
    "       [1., 0., 0.],\n",
    "       [1., 0., 0.],\n",
    "       [0., 0., 1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = K.constant(y_true, dtype=tf.float32)\n",
    "y_pred = K.constant(y_predicted, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49999994"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(f1_score(y_true=y_true, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binary classifiation f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true =    [0., 1., 0., 1., 0., 0.]\n",
    "y_predicted = [0., 1., 0., 0., 0., 1.]\n",
    "y_true = K.constant(y_true, dtype=tf.float32)\n",
    "y_pred = K.constant(y_predicted, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59999996"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(f_measure(y_true=y_true, y_pred=y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
